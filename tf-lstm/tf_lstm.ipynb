{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "PLOTTING_SUPPORT = True\n",
    "SET_EULER_PARAMETERS = False\n",
    "\n",
    "# Handle arguments (When executed as .py script)\n",
    "import sys\n",
    "argv = sys.argv[:]\n",
    "if len(argv) > 1:\n",
    "  script_path = argv.pop(0)\n",
    "  if \"--euler\" in argv:\n",
    "    SET_EULER_PARAMETERS = True\n",
    "    PLOTTING_SUPPORT = False\n",
    "    print(\"Parameters set for execution on euler cluster\")\n",
    "    argv.remove(\"--euler\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DATA_FOLDER = \"/home/daniel/Downloads/Data_200Hz/\"\n",
    "DATA_FILENAME=\"077_COSession1.set\"\n",
    "DATA2_FILENAME=\"077_COSession2.set\"\n",
    "ELECTRODES_OF_INTEREST = ['E36','E22','E9','E33','E24','E11','E124','E122','E45','E104',\n",
    "                          'E108','E58','E52','E62','E92','E96','E70','E83','E75']\n",
    "BATCH_SIZE = 100\n",
    "TRAINING_DATA_LENGTH = 1000\n",
    "VAL_DATA_LENGTH = 1000\n",
    "TEST_DATA_LENGTH = 1000\n",
    "SHUFFLE_TRAINING_EXAMPLES = False\n",
    "SAMPLING = 1\n",
    "OFFSET = 0\n",
    "\n",
    "MAX_STEPS = 1000\n",
    "\n",
    "VAL_EVERY_N_STEPS = 1\n",
    "VAL_STEP_TOLERANCE = 3\n",
    "\n",
    "class ModelParams:\n",
    "  def __init__(self):\n",
    "    self.BPTT_LENGTH = 100\n",
    "    self.NUM_UNITS = 128\n",
    "    self.N_LAYERS = 3\n",
    "    self.INPUT_SIZE = 19\n",
    "    self.OUTPUT_SIZE = 10\n",
    "    self.LEARNING_RATE = 0.001\n",
    "    self.CLIP_GRADIENTS = 1.0\n",
    "    self.SCALE_OUTPUT = 100.0\n",
    "    self.DROPOUT = 1.0\n",
    "  def __str__(self):\n",
    "    return str(self.__dict__)\n",
    "  def __eq__(self, other): \n",
    "    return self.__dict__ == other.__dict__\n",
    "  def __ne__(self, other):\n",
    "    return not self.__eq__(other)\n",
    "MP = ModelParams()\n",
    "\n",
    "SAVE_DIR = \"/home/daniel/Desktop/tf-lstm-model/\"\n",
    "SAVE_FILE = \"model.ckpt\"\n",
    "TENSORBOARD_DIR = \"/home/daniel/tensorboard\"\n",
    "#DATA_FOLDER = \"/home/daniel/Downloads/Raw-Waves/\"\n",
    "#DATA_FILENAME=\"001_Session1_FilterTrigCh_RawCh.mat\"\n",
    "#DATA2_FILENAME=\"001_Session2_FilterTrigCh_RawCh.mat\"\n",
    "#DATA3_FILENAME=\"034_Session1_FilterTrigCh_RawCh.mat\"\n",
    "\n",
    "assert MP.INPUT_SIZE == len(ELECTRODES_OF_INTEREST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if SET_EULER_PARAMETERS:\n",
    "    DATA_FOLDER = \"/cluster/home/dugasd/Data_200Hz/\"\n",
    "    SAVE_DIR = \"/cluster/home/dugasd/tf-lstm-model/\"\n",
    "    TENSORBOARD_DIR = None\n",
    "    \n",
    "    BATCH_SIZE = 10000\n",
    "    TRAINING_DATA_LENGTH = \"max\"\n",
    "    VAL_DATA_LENGTH = \"max\"\n",
    "    MAX_STEPS = 1000000\n",
    "    VAL_STEP_TOLERANCE = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if PLOTTING_SUPPORT:\n",
    "  import matplotlib.pyplot as plt\n",
    "  %matplotlib inline\n",
    "  from cycler import cycler\n",
    "  if SAMPLING > 0:\n",
    "      plotting_function = plt.step\n",
    "  else:\n",
    "      plotting_function = plt.plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SAVE_PATH = SAVE_DIR+SAVE_FILE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "  raw_wave = []\n",
    "  raw_wave2 = []\n",
    "  raw_wave3 = []\n",
    "\n",
    "  import scipy.io\n",
    "  mat = scipy.io.loadmat(DATA_FOLDER+DATA_FILENAME)\n",
    "  raw_wave = mat.get('data')[0]\n",
    "  raw_wave = raw_wave[::SAMPLING]\n",
    "  raw_wave = raw_wave[0:]\n",
    "\n",
    "  if DATA2_FILENAME is not None:\n",
    "      mat = scipy.io.loadmat(DATA_FOLDER+DATA2_FILENAME)\n",
    "      raw_wave2 = mat.get('data')[0]\n",
    "      raw_wave2 = raw_wave2[::SAMPLING]\n",
    "      raw_wave2 = raw_wave2[0:]\n",
    "  if DATA3_FILENAME is not None:\n",
    "      mat = scipy.io.loadmat(DATA_FOLDER+DATA3_FILENAME)\n",
    "      raw_wave3 = mat.get('data')[0]\n",
    "      raw_wave3 = raw_wave3[::SAMPLING]\n",
    "      raw_wave3 = raw_wave3[0:]\n",
    "    \n",
    "  # Save some memory\n",
    "  del mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "  raw_wave = []\n",
    "  raw_wave2 = []\n",
    "  raw_wave3 = []\n",
    " \n",
    "  import mne\n",
    "  raw_eeglab = mne.io.read_raw_eeglab(DATA_FOLDER+DATA_FILENAME)\n",
    "  electrode_names = raw_eeglab.ch_names\n",
    "  EOI_indices = [electrode_names.index(name) for name in ELECTRODES_OF_INTEREST]\n",
    "  raw_wave = np.array([raw_eeglab[e_index][0][0] for e_index in EOI_indices])\n",
    "  raw_wave = list(raw_wave.T)\n",
    "  raw_wave = raw_wave[::SAMPLING]\n",
    "\n",
    "  raw_eeglab = mne.io.read_raw_eeglab(DATA_FOLDER+DATA2_FILENAME)\n",
    "  electrode_names = raw_eeglab.ch_names\n",
    "  EOI_indices = [electrode_names.index(name) for name in ELECTRODES_OF_INTEREST]\n",
    "  raw_wave2 = np.array([raw_eeglab[e_index][0][0] for e_index in EOI_indices])\n",
    "  raw_wave2 = list(raw_wave2.T)\n",
    "  raw_wave2 = raw_wave2[::SAMPLING]\n",
    "\n",
    "  del raw_eeglab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "  np.save(DATA_FOLDER+\"raw_wave\", raw_wave)\n",
    "  np.save(DATA_FOLDER+\"raw_wave2\", raw_wave2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if True:\n",
    "  raw_wave  = np.load(DATA_FOLDER+\"raw_wave.npy\")\n",
    "  raw_wave2 = np.load(DATA_FOLDER+\"raw_wave2.npy\")\n",
    "  raw_wave  = raw_wave[::SAMPLING]\n",
    "  raw_wave2 = raw_wave2[::SAMPLING]\n",
    "  raw_wave  = raw_wave[OFFSET:]*1000000\n",
    "  raw_wave2 = raw_wave2[OFFSET:]*1000000\n",
    "  raw_wave3 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if TRAINING_DATA_LENGTH == \"max\":\n",
    "    TRAINING_DATA_LENGTH = len(raw_wave)\n",
    "if VAL_DATA_LENGTH == \"max\":\n",
    "    assert len(raw_wave2) != 0\n",
    "    VAL_DATA_LENGTH = len(raw_wave2) - TEST_DATA_LENGTH\n",
    "assert TRAINING_DATA_LENGTH + VAL_DATA_LENGTH + TEST_DATA_LENGTH <= len(raw_wave) + len(raw_wave2) + len(raw_wave3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Assign data to datasets.\n",
    "training_data = raw_wave[:TRAINING_DATA_LENGTH]\n",
    "if len(raw_wave2) is 0:\n",
    "  val_data = raw_wave[TRAINING_DATA_LENGTH:][:VAL_DATA_LENGTH]\n",
    "  test_data = raw_wave[TRAINING_DATA_LENGTH:][VAL_DATA_LENGTH:][:TEST_DATA_LENGTH]\n",
    "else:\n",
    "  val_data = raw_wave2[:VAL_DATA_LENGTH]\n",
    "  if len(raw_wave3) is 0:\n",
    "    test_data = raw_wave2[VAL_DATA_LENGTH:][:TEST_DATA_LENGTH]\n",
    "  else:\n",
    "    test_data = raw_wave3[:TEST_DATA_LENGTH]\n",
    "\n",
    "\n",
    "if PLOTTING_SUPPORT:\n",
    "  plt.figure(figsize=(500,10))\n",
    "  plotting_function(range(TRAINING_DATA_LENGTH),training_data[:,0],label=\"training\")\n",
    "  plotting_function(range(TRAINING_DATA_LENGTH,TRAINING_DATA_LENGTH+VAL_DATA_LENGTH),val_data[:,0],label=\"validation\")\n",
    "  plotting_function(range(TRAINING_DATA_LENGTH+VAL_DATA_LENGTH,\n",
    "                 TRAINING_DATA_LENGTH+VAL_DATA_LENGTH+TEST_DATA_LENGTH),test_data[:,0],label=\"test\")\n",
    "  plt.ylim([-1000,1000])\n",
    "  plt.figure(figsize=(500,10))\n",
    "  plotting_function(range(TRAINING_DATA_LENGTH),training_data[:,0],label=\"training\")\n",
    "  plotting_function(range(TRAINING_DATA_LENGTH,TRAINING_DATA_LENGTH+VAL_DATA_LENGTH),val_data[:,0],label=\"validation\")\n",
    "  plotting_function(range(TRAINING_DATA_LENGTH+VAL_DATA_LENGTH,\n",
    "                 TRAINING_DATA_LENGTH+VAL_DATA_LENGTH+TEST_DATA_LENGTH),test_data[:,0],label=\"test\")\n",
    "  plt.xlim([0, 100000])\n",
    "  plt.ylim([-100,100])\n",
    "  #plt.legend()\n",
    "print(len(raw_wave)-TRAINING_DATA_LENGTH)\n",
    "print(len(raw_wave2)-VAL_DATA_LENGTH-TEST_DATA_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "if os.path.exists(SAVE_PATH):\n",
    "  with open(SAVE_DIR+\"model_params.pckl\", 'rb') as file:\n",
    "    import pickle\n",
    "    mp_loaded = pickle.load(file)\n",
    "  if MP != mp_loaded:\n",
    "    print(MP)\n",
    "    print(mp_loaded)\n",
    "  assert MP == mp_loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Create Graph\n",
    "tf.reset_default_graph()\n",
    "\n",
    "preset_batch_size = None\n",
    "# Placeholders\n",
    "with tf.name_scope(\"input_placeholders\") as scope:\n",
    "  input_placeholders = [tf.placeholder(tf.float32, shape=(preset_batch_size, MP.INPUT_SIZE), name=\"input\"+str(i))\n",
    "                        for i in range(MP.BPTT_LENGTH)]\n",
    "dropout_placeholder = tf.placeholder(tf.float32, name=\"dropout_prob\")\n",
    "c_state_placeholders = [tf.placeholder(tf.float32, shape=(preset_batch_size, MP.NUM_UNITS), name=\"c_state\"+str(i)) for i in range(MP.N_LAYERS)]\n",
    "h_state_placeholders = [tf.placeholder(tf.float32, shape=(preset_batch_size, MP.NUM_UNITS), name=\"h_state\"+str(i)) for i in range(MP.N_LAYERS)]\n",
    "initial_state = tuple(tf.nn.rnn_cell.LSTMStateTuple(c_state, h_state) for c_state, h_state in zip(c_state_placeholders, h_state_placeholders))\n",
    "\n",
    "stacked_lstm = tf.nn.rnn_cell.MultiRNNCell(\n",
    "    [tf.nn.rnn_cell.DropoutWrapper(tf.nn.rnn_cell.LSTMCell(MP.NUM_UNITS, state_is_tuple=True),\n",
    "                                   output_keep_prob=dropout_placeholder)] * MP.N_LAYERS,\n",
    "                                           state_is_tuple=True)\n",
    "unrolled_outputs, state = tf.nn.rnn(stacked_lstm, input_placeholders, dtype=tf.float32, initial_state=initial_state)\n",
    "\n",
    "outputs = [tf.mul(cell_output[:, 0:MP.OUTPUT_SIZE], tf.constant(MP.SCALE_OUTPUT)) for cell_output in unrolled_outputs]\n",
    "\n",
    "target_placeholder = tf.placeholder(tf.float32, shape=(preset_batch_size, MP.OUTPUT_SIZE), name=\"target\")\n",
    "\n",
    "loss = tf.square(target_placeholder - outputs[-1], name=\"loss\")\n",
    "if MP.OUTPUT_SIZE > 1:\n",
    "    loss = tf.reduce_sum(loss, 1) # add together loss for all outputs\n",
    "cost = tf.reduce_mean(loss, name=\"cost\")   # average over batch\n",
    "# Use ADAM optimizer\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=MP.LEARNING_RATE).minimize(cost)\n",
    "if MP.CLIP_GRADIENTS > 0:\n",
    "  adam = tf.train.AdamOptimizer(learning_rate=MP.LEARNING_RATE)\n",
    "  gvs = adam.compute_gradients(cost)\n",
    "#  capped_gvs = [(tf.clip_by_value(grad, -MP.CLIP_GRADIENTS, MP.CLIP_GRADIENTS), var) for grad, var in gvs]\n",
    "  capped_gvs = [(tf.clip_by_norm(grad, MP.CLIP_GRADIENTS), var) for grad, var in gvs]\n",
    "  optimizer = adam.apply_gradients(capped_gvs)\n",
    "\n",
    "# Initialize session and write graph for visualization.\n",
    "sess = tf.Session()\n",
    "tf.initialize_all_variables().run(session=sess)\n",
    "if TENSORBOARD_DIR != None:\n",
    "  summary_writer = tf.train.SummaryWriter(TENSORBOARD_DIR, sess.graph)\n",
    "  print(\"Tensorboard graph saved.\")\n",
    "\n",
    "print(\"Session created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()\n",
    "\n",
    "import pickle\n",
    "mp_filename = \"model_params.pckl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Restore model weights from previously saved model\n",
    "import os\n",
    "if os.path.exists(SAVE_PATH):\n",
    "  saver.restore(sess, SAVE_PATH)\n",
    "  print(\"Model restored from file: %s\" % SAVE_PATH)\n",
    "else:\n",
    "  print(\"No model found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from batchmaker import StatefulBatchmaker\n",
    "\n",
    "total_step_cost = None\n",
    "step_cost_log = []\n",
    "total_val_cost = 0\n",
    "val_cost_log = []\n",
    "val_steps_since_last_improvement = 0\n",
    "\n",
    "# single step\n",
    "for step in range(MAX_STEPS):\n",
    "  # Validation\n",
    "  val_batchmaker = StatefulBatchmaker(val_data, MP.BPTT_LENGTH, BATCH_SIZE, MP.OUTPUT_SIZE)\n",
    "  prev_batch_c_states = [np.zeros((BATCH_SIZE, MP.NUM_UNITS)) for i in range(MP.N_LAYERS)]\n",
    "  prev_batch_h_states = [np.zeros((BATCH_SIZE, MP.NUM_UNITS)) for i in range(MP.N_LAYERS)]\n",
    "  if np.mod(step, VAL_EVERY_N_STEPS) == 0:\n",
    "    total_val_cost = 0\n",
    "    while True:\n",
    "      if val_batchmaker.is_depleted():\n",
    "        break\n",
    "      else:\n",
    "        batch_input_values, batch_target_values = val_batchmaker.next_batch()\n",
    "    \n",
    "        # Assign a value to each placeholder.\n",
    "        feed_dictionary = {ph: v for ph, v in zip(input_placeholders, batch_input_values)}\n",
    "        feed_dictionary[target_placeholder] = batch_target_values\n",
    "        feed_dictionary[dropout_placeholder] = 1.0\n",
    "        for c_state_placeholder, prev_batch_c_state in zip(c_state_placeholders, prev_batch_c_states):\n",
    "            feed_dictionary[c_state_placeholder] = prev_batch_c_state\n",
    "        for h_state_placeholder, prev_batch_h_state in zip(h_state_placeholders, prev_batch_h_states):\n",
    "            feed_dictionary[h_state_placeholder] = prev_batch_h_state\n",
    "\n",
    "       # Validate.\n",
    "        cost_value, state_value = sess.run((cost, state), feed_dict=feed_dictionary)\n",
    "        total_val_cost += cost_value\n",
    "        prev_batch_c_states = [state_value[i].c for i in range(len(state_value))]\n",
    "        prev_batch_h_states = [state_value[i].h for i in range(len(state_value))]\n",
    "    print(\"Validation cost: \", end='')\n",
    "    print(total_val_cost, end='')\n",
    "    print(\"  (Training cost: \", end='')\n",
    "    print(total_step_cost, end='')\n",
    "    print(\")\")\n",
    "    val_cost_log.append(total_val_cost)\n",
    "    \n",
    "    # Training Monitor\n",
    "    if len(val_cost_log) > 1:\n",
    "        # Save cost log.\n",
    "        import os\n",
    "        if not os.path.exists(SAVE_DIR):\n",
    "            os.makedirs(SAVE_DIR)\n",
    "            print(\"Created directory: %s\" % SAVE_DIR)\n",
    "        np.savetxt(SAVE_DIR+\"val_cost_log.txt\", val_cost_log)\n",
    "        # Save if cost has improved. Otherwise increment counter.\n",
    "        if val_cost_log[-1] <  min(val_cost_log[:-1]):\n",
    "            val_steps_since_last_improvement = 0\n",
    "            # save model to disk\n",
    "            print(\"Saving ... \", end='')\n",
    "            save_path = saver.save(sess, SAVE_PATH)\n",
    "            with open(SAVE_DIR+mp_filename, 'wb') as file:\n",
    "              pickle.dump(MP, file, protocol=2)\n",
    "            print(\"Model saved in file: %s\" % save_path)      \n",
    "        else:\n",
    "            val_steps_since_last_improvement += 1         \n",
    "    # Stop training if val_cost hasn't improved in VAL_STEP_TOLERANCE steps\n",
    "    if val_steps_since_last_improvement > VAL_STEP_TOLERANCE:\n",
    "        print(\"Training stopped by validation monitor.\")\n",
    "        break\n",
    "            \n",
    "  # Train on batches\n",
    "  training_batchmaker = StatefulBatchmaker(training_data, MP.BPTT_LENGTH, BATCH_SIZE, MP.OUTPUT_SIZE)\n",
    "  total_step_cost = 0\n",
    "  prev_batch_c_states = [np.zeros((BATCH_SIZE, MP.NUM_UNITS)) for i in range(MP.N_LAYERS)]\n",
    "  prev_batch_h_states = [np.zeros((BATCH_SIZE, MP.NUM_UNITS)) for i in range(MP.N_LAYERS)]\n",
    "  while True:\n",
    "    if training_batchmaker.is_depleted():\n",
    "      break\n",
    "    else:\n",
    "      batch_input_values, batch_target_values = training_batchmaker.next_batch()\n",
    "      \n",
    "      # Assign a value to each placeholder.\n",
    "      feed_dictionary = {ph: v for ph, v in zip(input_placeholders, batch_input_values)}\n",
    "      feed_dictionary[target_placeholder] = batch_target_values\n",
    "      feed_dictionary[dropout_placeholder] = MP.DROPOUT\n",
    "      for c_state_placeholder, prev_batch_c_state in zip(c_state_placeholders, prev_batch_c_states):\n",
    "        feed_dictionary[c_state_placeholder] = prev_batch_c_state\n",
    "      for h_state_placeholder, prev_batch_h_state in zip(h_state_placeholders, prev_batch_h_states):\n",
    "        feed_dictionary[h_state_placeholder] = prev_batch_h_state\n",
    "  \n",
    "      # Train over 1 batch.\n",
    "      opt_value, last_output_value, cost_value, state_value = sess.run((optimizer, outputs[-1], cost, state),\n",
    "                                                              feed_dict=feed_dictionary)\n",
    "      total_step_cost += cost_value\n",
    "      prev_batch_c_states = [state_value[i].c for i in range(len(state_value))]\n",
    "      prev_batch_h_states = [state_value[i].h for i in range(len(state_value))]\n",
    "      assert not np.isnan(last_output_value).any()\n",
    "  step_cost_log.append(total_step_cost)\n",
    "\n",
    "\n",
    "print(\"Training ended.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if PLOTTING_SUPPORT:\n",
    "  plt.figure(figsize=(100,10))\n",
    "  plotting_function(range(len(step_cost_log)), step_cost_log, label=\"step_cost_log\")\n",
    "  plotting_function(range(len(val_cost_log)), val_cost_log, label=\"val_cost_log\")\n",
    "  plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Restore model weights from previously saved model\n",
    "import os\n",
    "if os.path.exists(SAVE_PATH):\n",
    "  saver.restore(sess, SAVE_PATH)\n",
    "  print(\"Model restored from file: %s\" % SAVE_PATH)\n",
    "else:\n",
    "  print(\"No model found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "offset = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "REALIGN_OUTPUT = True\n",
    "\n",
    "from batchmaker import StatefulBatchmaker\n",
    "test_batchmaker = StatefulBatchmaker(test_data, MP.BPTT_LENGTH, 1, MP.OUTPUT_SIZE, True)\n",
    "\n",
    "\n",
    "testing_cost = 0\n",
    "prev_batch_c_states = [np.zeros((1, MP.NUM_UNITS)) for i in range(len(state_value))]\n",
    "prev_batch_h_states = [np.zeros((1, MP.NUM_UNITS)) for i in range(len(state_value))]\n",
    "while True:\n",
    "  if test_batchmaker.is_depleted():\n",
    "    break\n",
    "  else:\n",
    "    batch_input_values, batch_target_values = test_batchmaker.next_batch()\n",
    "    \n",
    "    # Assign a value to each placeholder.\n",
    "    feed_dictionary = {ph: v for ph, v in zip(input_placeholders, batch_input_values)}\n",
    "    feed_dictionary[target_placeholder] = batch_target_values\n",
    "    feed_dictionary[dropout_placeholder] = 1.0\n",
    "    for c_state_placeholder, prev_batch_c_state in zip(c_state_placeholders, prev_batch_c_states):\n",
    "      feed_dictionary[c_state_placeholder] = prev_batch_c_state\n",
    "    for h_state_placeholder, prev_batch_h_state in zip(h_state_placeholders, prev_batch_h_states):\n",
    "      feed_dictionary[h_state_placeholder] = prev_batch_h_state\n",
    "\n",
    "    # Test over 1 batch.\n",
    "    last_output_value, cost_value, state_value = sess.run((outputs[-1], cost, state), feed_dict=feed_dictionary)\n",
    "    testing_cost += cost_value\n",
    "    prev_batch_c_states = [state_value[i].c for i in range(len(state_value))]\n",
    "    prev_batch_h_states = [state_value[i].h for i in range(len(state_value))]\n",
    "    assert not np.isnan(last_output_value).any()   \n",
    "\n",
    "if PLOTTING_SUPPORT:\n",
    "  plt.figure(figsize=(TEST_DATA_LENGTH/20,10))\n",
    "  plt.gca().set_prop_cycle(cycler('color', ['k'] + [(1,w,w) for w in np.linspace(0.8,0,MP.OUTPUT_SIZE)]))\n",
    "  plot_data = np.array(test_data)\n",
    "  if MP.INPUT_SIZE > 1:\n",
    "    plot_data = plot_data[:,0]\n",
    "  plotting_function(range(len(plot_data)), plot_data, label=\"test data\")\n",
    "  if REALIGN_OUTPUT:\n",
    "    abscisses = np.tile(np.arange(MP.BPTT_LENGTH, MP.BPTT_LENGTH+len(output_value))[:,None], (1,MP.OUTPUT_SIZE))\n",
    "    abscisses = abscisses + np.arange(MP.OUTPUT_SIZE)\n",
    "  else:\n",
    "    abscisses = np.arange(MP.BPTT_LENGTH, MP.BPTT_LENGTH+len(output_value))\n",
    "  plotting_function(abscisses, output_value, label=\"prediction\")\n",
    "  plt.legend()\n",
    "  if MP.INPUT_SIZE > 1:\n",
    "    plt.figure(figsize=(TEST_DATA_LENGTH/20,10))\n",
    "    plot_data = np.array(test_data)[:,1:]\n",
    "    plotting_function(range(len(plot_data)), plot_data, label=\"electrodes\")\n",
    "    plt.legend()  \n",
    "  print(\"Offset: \", end='')\n",
    "  print(offset)\n",
    "print(\"Testing cost: \", end='')\n",
    "print(testing_cost)\n",
    "\n",
    "#Reset test data to normal data\n",
    "offset += TEST_DATA_LENGTH\n",
    "if len(raw_wave2) is 0:\n",
    "  test_data = raw_wave[offset:][TRAINING_DATA_LENGTH:][VAL_DATA_LENGTH:][:TEST_DATA_LENGTH]\n",
    "else:\n",
    "  if len(raw_wave3) is 0:\n",
    "    test_data = raw_wave2[offset:][VAL_DATA_LENGTH:][:TEST_DATA_LENGTH]\n",
    "  else:\n",
    "    test_data = raw_wave3[offset:][:TEST_DATA_LENGTH]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "  if PLOTTING_SUPPORT:\n",
    "    from IPython import display\n",
    "    for i in range(output_value.shape[0]):\n",
    "      plotting_function(range(10), batch_target_values[i], label='target')\n",
    "      plotting_function(range(10), output_value[i], label='prediction')\n",
    "      plt.legend()\n",
    "      plt.ylim([-10,10])\n",
    "      plt.show()\n",
    "      plt.pause(0.01)\n",
    "      display.clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Replace test data with sine wave\n",
    "test_data = 30*np.sin(np.linspace(0,100*np.pi,TEST_DATA_LENGTH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Reset test data to normal data\n",
    "offset += TEST_DATA_LENGTH\n",
    "if len(raw_wave2) is 0:\n",
    "  test_data = raw_wave[offset:][TRAINING_DATA_LENGTH:][VAL_DATA_LENGTH:][:TEST_DATA_LENGTH]\n",
    "else:\n",
    "  if len(raw_wave3) is 0:\n",
    "    test_data = raw_wave2[offset:][VAL_DATA_LENGTH:][:TEST_DATA_LENGTH]\n",
    "  else:\n",
    "    test_data = raw_wave3[offset:][:TEST_DATA_LENGTH]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hallucination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "HALLUCINATION_LENGTH = 200\n",
    "HALLUCINATION_FUTURE = 4\n",
    "\n",
    "from batchmaker import StatefulBatchmaker\n",
    "hal_batchmaker = StatefulBatchmaker(test_data, MP.BPTT_LENGTH, 1, MP.OUTPUT_SIZE, True)\n",
    "prev_batch_c_states = [np.zeros((1, MP.NUM_UNITS)) for i in range(len(state_value))]\n",
    "prev_batch_h_states = [np.zeros((1, MP.NUM_UNITS)) for i in range(len(state_value))]\n",
    "batch_input_values, batch_target_values = hal_batchmaker.next_batch()\n",
    "\n",
    "hal_output = []\n",
    "for i in range(HALLUCINATION_LENGTH):\n",
    "  # Assign a value to each placeholder.\n",
    "  feed_dictionary = {ph: v for ph, v in zip(input_placeholders, batch_input_values)}\n",
    "  feed_dictionary[target_placeholder] = batch_target_values\n",
    "  feed_dictionary[dropout_placeholder] = 1.0\n",
    "  for c_state_placeholder, prev_batch_c_state in zip(c_state_placeholders, prev_batch_c_states):\n",
    "    feed_dictionary[c_state_placeholder] = prev_batch_c_state\n",
    "  for h_state_placeholder, prev_batch_h_state in zip(h_state_placeholders, prev_batch_h_states):\n",
    "    feed_dictionary[h_state_placeholder] = prev_batch_h_state\n",
    "\n",
    "  # Run session\n",
    "  output_value, state_value = sess.run((outputs[-1], state), feed_dict=feed_dictionary)\n",
    "  hal_output.append(output_value[0,HALLUCINATION_FUTURE])\n",
    "\n",
    "  prev_batch_c_states = [state_value[i].c for i in range(len(state_value))]\n",
    "  prev_batch_h_states = [state_value[i].h for i in range(len(state_value))]\n",
    "\n",
    "  batch_input_values, batch_target_values = hal_batchmaker.next_batch()\n",
    "  batch_input_values[-1][0,0] = output_value[0,0]\n",
    "\n",
    "if PLOTTING_SUPPORT:\n",
    "  plt.figure(figsize=(20,10))\n",
    "  plotting_function(range(len(test_data)), np.array(test_data)[:,0], label=\"test data\")\n",
    "  plotting_function(range(MP.BPTT_LENGTH,MP.BPTT_LENGTH+len(hal_output)),hal_output, label=\"prediction\")\n",
    "  plt.xlim([0,MP.BPTT_LENGTH+len(hal_output)])\n",
    "  plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## You've got mail!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(DATA_FOLDER+\"secret.txt\") as file:\n",
    "    secret=file.read().replace('\\n', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if not SET_EULER_PARAMETERS:\n",
    "  import smtplib\n",
    "   \n",
    "  server = smtplib.SMTP('smtp.gmail.com', 587)\n",
    "  server.starttls()\n",
    "  server.login(\"brainwavesdev@gmail.com\", secret)\n",
    " \n",
    "  msg = \"Waves brained!\"\n",
    "  server.sendmail(\"brainwavesdev@gmail.com\", \"brainwavesdev@gmail.com\", msg)\n",
    "  server.quit()\n",
    "\n",
    "  print(\"Mail sent.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Author: Daniel Dugas"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [ml3]",
   "language": "python",
   "name": "Python [ml3]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
